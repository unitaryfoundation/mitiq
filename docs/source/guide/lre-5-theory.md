---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.4
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---


```{warning}
The user guide for LRE in Mitiq is currently under construction.
```

# What is the theory behind LRE?

Layerwise Richardson Extrapolation (LRE), an error mitigation technique, introduced in
{cite}`Russo_2024_LRE` works by creating multiple noise-scaled variations of the input
circuit such that the noiseless expectation value is extrapolated from the execution of each
noisy circuit.

Similar to [ZNE](zne.md), this process works in two steps:

- **Step 1:** Intentionally create multiple noise-scaled but logically equivalent circuits through unitary folding.

- **Step 2:** Extrapolate to the noiseless limit using multivariate richardson extrapolation.

LRE leverages the flexible configurational space of layerwise unitary folding,
allowing for a more nuanced mitigation of errors by treating the noise level of each layer of
the quantum circuit as an independent variable.

## Step 1: Intentionally create multiple noise-scaled but logically equivalent circuits

The goal is to create noise-scaled circuits of different depths where the layers in each circuit are scaled in
a specific pattern as a result of unitary folding. This pattern is often described by the vector of scale factor vectors
generated by the fold multiplier and the chosen degree for multivariate Richardson extrapolation polynomial. For more information
on unitary folding, go to [What is the theory behind ZNE?](zne-5-theory.md).

Suppose we want to estimate the noiseless expectation value of some observable in an $n$-qubit circuit with $l$ layers.

Each layer can have a different scale factor and we can create $M$ such variations of the scaled circuit. Let $\{λ_1, λ_2, λ_3, \ldots, λ_M\}$ be the scale factors vectors used to create multiple variations of the noise-scaled circuits $\{C_{λ_1}, C_{λ_2}, C_{λ_3}, \ldots, C_{λ_M}\}$ such that each vector $λ_i$ defines the scale factors for the different layers in the input circuit $\{{λ^1}_i, {λ^2}_i, {λ^3}_i, \ldots, {λ^l}_i\}^T$.

If $d$ is the chosen degree of our multivariate polynomial, $M_j(λ_i, d)$ corresponds to the terms in the polynomial. In general, the monomial terms for a variable $l$ up to degree $d$ can be determined through the [stars and bars method](https://en.wikipedia.org/wiki/Stars_and_bars_%28combinatorics%29).

$$
\text{total number of terms in the monomial basis with max degree } d = \binom{d + l}{d}
$$

$$
\text{number of terms in the monomial basis with total degree } d = \binom{d + l - 1}{d}
$$

These monomial terms define the rows of the square sample matrix where $a_{i,j}=M_j(λ_i, d)$.

$$
\mathbf{A}(\Lambda, d) = 
\begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,M} \\
    a_{2,1} & a_{2,2} & \cdots & a_{2,M} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{N,1} & a_{N,2} & \cdots & a_{N,M}
\end{bmatrix}
$$

Each monomial term in the sample matrix is evaluated using the values in the scale factor vectors. We aim to define the zero-noise limit as a linear combination of the noisy expectation values. Finding the coefficients in the linear combination becomes a problem solvable through a system of linear equations $Ac = z$ where $c$ is the coefficients vector, $z$ is the vector of expectation values and $\mathbf{A}$ is the sample matrix evaluated using the values in the scale factor vectors.

## Step 2: Extrapolate to the noiseless limit

Each noise scaled circuit $C_{λ_i}$ has an expectation value associated with it $\langle O(λ_i) \rangle$ such that we can define a vector of the noisy expectation values $z = (\langle O(λ_1) \rangle, \langle O(λ_2) \rangle, \langle O(λ_3) \rangle, \ldots, \langle O(λ_M)\rangle)^T$. These have a coefficient of linear combination associated with them such that 

$$
O_{LRE} = \sum_{i=1}^{M} \eta_i \langle O(\boldsymbol{\lambda}_i) \rangle.
$$

The system of linear equations is used to find the numerous $\eta_i$. As we only need to find the noiseless expectation value, we do not need to calculate the full vector of linear combination coefficients if we use the [Lagrange interpolation formula](https://files.eric.ed.gov/fulltext/EJ1231189.pdf). 

$$
O_{\rm LRE} = \sum_{i=1}^M \langle O (\boldsymbol{\lambda}_i)\rangle  \frac{\det \left(\mathbf{M}_i (\boldsymbol{0}) \right)}{\det \left(\mathbf{A}\right)}.
$$

To get the matrix $\mathbf{M}_i(\mathbf{0})$, replace the $i$-th row of the sample matrix $\mathbf{A}$ by $\mathbf{e}_1=(1, 0, \ldots, 0)^T$.
